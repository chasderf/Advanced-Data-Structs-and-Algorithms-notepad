12.4 Algorithms for Solving Nonlinear Equations 


In this section, we discuss several algorithms for solving nonlinear equations in one unknown,	
			f(x) = 0. 

There are several reasons for this choice among subareas of numerical analysis. First of all, this is an extremely important problem from the practical and theoretical points of view. It arises as a mathematical model of numerous phenomena in the sciences and engineering, both directly and indirectly. (Recall, for example, that the standard calculus technique for finding extremum points of a function f(x) is based on finding its critical points, which are the roots of the equation J'(x) = 0.) Second, it represents the most accessible topic in numerical analysis and, at the same time, exhibits its typical tools and concerns. Third, some methods for solving equations closely parallel algorithms for array searching and hence provide examples of applying general algorithm design techniques to problems of
continuous mathematics. 
	Let us start with dispelling a misconception you might have about solving equations. Your experience with equation solving from middle school to calculus courses might have led you to believe that we can solve equations by "factoring" or by applying a readily available formula. Sorry to break it to you, but you have been deceived (with the best of educational intentions, of course): you were able to solve all those equations only because they had been carefully selected to make
it possible. In general, we cannot solve equations exactly and need approximation algorithms to do so. 
	In addition, as we discussed in Section 11.4, this canonical
formula needs to be modified to avoid the possibility of low-accuracy answers. What about formulas for roots of polynomials of degrees higher than two? Such formulas for third- and fourth-degree polynomials exist, but they are too cumbersome to be of practical value. For polynomials of degrees higher than four, there can be no general formula for their roots that would involve only the polynomial's coefficients, arithmetical operations, and radicals (taking roots). This
remarkable result was published first by the Italian mathematician and physician Paolo Ruffini (1765-1822) in 1799 and rediscovered a quarter century later by the Norwegian mathematician Niels Abel (1802-1829); it was developed further by the French mathematician Evariste Galois (1811-1832).
	The impossibility of such a formula can hardly be considered a great disappointment. As the great German mathematician Carl Friedrich Gauss (1777-1855) put it in his thesis of 1801, the algebraic solution of an equation was no better than devising a symbol for the root of the equation and then saying that the equation had a root equal to the symbol 

Bisection Method
This algorithm is based on an observation that the graph of a continuous function must intersect with the x-axis between two points a and b at least once if the function's values have opposite signs at these two points.
The validity of this observation is proved as a theorem in calculus courses, and we take it for granted here. It serves as the basis of the following algorithm, called the bisection method, for solving equation (12.4). Starting with an interval [a, b] at whose endpoints f(x) has opposite signs, the algorithm computes the value of.f(x) at the middle point x,;d = (a+ h)/2. If .f (x,;d) = 0, a root was found and the algorithm stops. Otherwise, it continues the search for a root either on [a, xmid] or on [xmidâ€¢ b ], depending on which of the two halves the values of .f(x) have opposite signs at the endpoints of the new interval. 

ALGORITHM Bisection(f(x), a, h, eps, N)
//Implements the bisection method for finding a root of f(x) = 0
//Input: Two real numbers a and b, a< h,
II a continuous function f(x) on [a, b], f(a)f(b) < 0,
II an upper bound on the absolute error eps > 0,
II an upper bound on the number of iterations N
//Output: An approximate (or exact) value x of a root in (a, b)
//or an interval bracketing tbe root if the iteration number limit is reached
n +-- 1 //iteration count
while n :<: N do
x +--(a+ b)/2
ifx- a< eps returnx
fval +-- f(x)
if.fval = 0 return x
if.fval * f(a) < 0
b+--x
else a +- x
n+--n+1
return "iteration limit", a, b 


Method of false Position
The method of false position (also known by its name in Latin, regula falsi) is to interpolation search as the bisection method is to binary search. Like the bisection method, it has, on each iteration, some interval [a,, b,] bracketing a root of a continuous function f(x) that has opposite-sign values at a, and b,. Unlike the bisection method, however, it computes the next root approximation not as the middle of [a,, b,] but as the x-intercept of the straight line through the points
(a,, f(a,)) and (b,, f(b,)) 

NEWTONS METHOD
Newton's method, also called the Newton-Raphson method, is one of the most important general algorithms for solving equations. When applied to equation (12.4) in one unknown, it can be illustrated by Figure 12.22: the next element xn+l of the method's approximation sequence is obtained as the x-intercept of the tangent line to the graph of function f (x) at xn .






























