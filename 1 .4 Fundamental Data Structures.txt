1 .4 Fundamental Data Structures
Since the vast majority of algorithms of interest operate on data, particular ways of organizing data play a critical role in the design and analysis of algorithms. A data structure can be defined as a particular scheme of organizing related data items. The nature of the data items is dictated by a problem at hand; they can range from elementary data types (e.g., integers or characters) to data structures (e.g., a
one-dimensional array of one-dimensional arrays is often used for implementing matrices). There are a few data structures that have proved to be particularly important for computer algorithms. Since you are undoubtedly familiar with most if not all of them, just a quick review is provided here. 

Linear Data Structures
The two most important elementary data structures are the array and the linked list. A (one-dimensional) array is a sequence of n items of the same data type that are stored contiguously in computer memory and made accessible by specifying a value of the array's index 



Arrays are used for implementing a variety of other data structures. Prominent among them is the string, a sequence of characters from an alphabet terminated by a special character indicating the string's end. Strings composed of zeros and ones are called binary strings or bit strings. Strings are indispensable for processing textual data, defining computer languages and compiling programs written in them, and studying abstract computational models. Operations we usually perform on strings differ from those we typically perform on other arrays (say, arrays
of numbers). They include computing the string length, comparing two strings to determine which one precedes the other according to the so-called lexicographic order, i.e., in a dictionary, and concatenating two strings (forming one string from two given strings by appending the second to the end of the first). A linked list is a sequence of zero or more elements called nodes each containing two kinds of information: some data and one or more links called pointers to other nodes of the linked list. (A special pointer called "null" is used to indicate the absence of a node's successor.) In a singly linked list, each node
except the last one contains a single pointer to the next element (Figure 1.4). To access a particular node of a linked list, we start with the list's first node and traverse the pointer chain until the particular node is reached. Thus, the time needed to access an element of a singly linked list, unlike that of an array, depends on where in the list the element is located. On the positive side, linked lists do not require any preliminary reservation of the computer memory, and insertions and deletions can be made quite efficiently in a linked list by reconnecting a few appropriate pointers. We can exploit flexibility of the linked list structure in a variety of ways. For example, it is often convenient to start a linked list with a special node called
the header. This node often contains information about the linked list such as its current length; it may also contain, in addition to a pointer to the first element, a pointer to the linked list's last element. Another extension is the structure called the doubly linked list, in which every node, except the first and the last, contains pointers to both its successor and its predecessor.

The array and linked list are two principal choices in representing a more abstract data structure called a linear list or simply a list. A list is a finite sequence of data items, i.e., a collection of data items arranged in a certain linear order. The basic operations performed on this data structure are searching for, inserting, and
deleting an element. 

Two special types of lists, stacks and queues, are particularly important. A stack is a list in which insertions and deletions can be done only at the end. This end is called the top because a stack is usually visualized not horizontally but vertically (akin to a stack of plates whose "operations" it mimics very closely). As a result, when elements are added to (pushed onto) a stack and deleted from (popped off) it, the structure operates in the "last-in-first-out" (LIFO) fashion, exactly as the stack of plates does if we can remove only the top plate or add another plate to top of the stack. Stacks have a multitude of applications; in particular, they are indispensable for implementing recursive algorithms. 

A queue, on the other hand, is a list from which elements are deleted from one end of the structure, called the front (this operation is called dequeue), and new elements are added to the other end, called the rear (this operation is called enqueue). Consequently, a queue operates in the "first -in-first-out" (FIFO) fashion (akin, say, to a queue of customers served by a single teller in a bank). Queues also have many important applications, including several algorithms for graph problems. 


Many important applications require selection of an item of the highest priority among a dynamically changing set of candidates. A data structure that seeks to satisfy the needs of such applications is called a priority queue. A priority queue is a collection of data items from a totally ordered universe (most often, integer or real numbers). The principal operations on a priority queue are finding its largest element, deleting its largest element, and adding a new element. Of course, a priority queue must be implemented so that the last two operations yield another priority queue. Straightforward implementations of this data structure can be based on either an array or a sorted array, but neither of these options yields the most efficient solution possible. A better implementation of a priority queue is based on an ingenious data structure called the heap. We discuss heaps (and an
important sorting algorithm based on them) in Section 6.4. 


Graphs
As mentioned in the previous section, a graph is informally thought of as a collection of points in the plane called "vertices" or "nodes," some of them connected by line segments called "edges" or "arcs." Formally, a graph G = {V, E) is defined by a pair of two sets: a finite set V of items called vertices and a set E of pairs of these items called edges. lf these pairs of vertices are unordered, i.e., a pair of
vertices (u, v) is the same as the pair (v, u), we say that the vertices u and v are adjacent to each other and that they are connected by the undirected edge (u, v). We call the vertices u and v endpoints of the edge (u, v) and say that u and v are incident to this edge; we also say that the edge (u, v) is incedent to its endpoints
u and v. A graph G is called undirected if every edge in it is undirected.

If a pair of vertices (u, v) is not the same as the pair (v, u), we say that the edge (u, v) is directed from the vertex u, called the edge's tail, to the vertex v, called the edge's head. We also say that the edge (u, v) leaves u and enters v. A graph whose every edge is directed is called directed. Directed graphs are also called digraphs. 

Graph representations Graphs for computer algorithms can be represented in two principal ways: the adjacency matrix and adjacency lists. The adjacency matrix of a graph with n vertices is an n-by-n boolean matrix with one row and one column for each of the graph's vertices, in which the element in the ith row and the jth column is equal to 1 if there is an edge from the ith vertex to the jth vertex, and equal to 0 if there is no such edge. For example, the adjacency matrix for the graph in Figure 1.6a is given in Figure 1.7a. Note that the adjacency
matrix of an undirected graph is always symmetric, i.e., A[i, j] = A[j, i] for every 0 :0 i, j :0 n - 1 (why?). 

The adjacency lists of a graph or a digraph is a collection of linked lists, one for each vertex, that contain all the vertices adjacent to the list's vertex (i.e., all the vertices connected to it by an edge). Usually, such lists start with a header identifying a vertex for which the list is compiled. For example, Figure 1.7b represents the graph in Figure 1.6a via its adjacency lists. To put it another way, adjacency lists indicate columns of the adjacency matrix that, for a given vertex,
contain 1 's. 

Weighted graphs A weighted graph (or weighted digraph) is a graph (or digraph) with numbers assigned to its edges. These numbers are called weights or costs. An interest in such graphs is motivated by numerous real-life applications, such as finding the shortest path between two points in a transportation or communication network or the traveling salesman problem mentioned earlier. 

Paths and cycles Among many interesting properties of graphs, two are important for a great number of applications: connectivity and acyclicity. Both are based on the notion of a path. A path from vertex u to vertex v of a graph G can be detined as a sequence of adjacent (connected by an edge) vertices that starts with u and ends with v. If all vertices of a path are distinct, the path is said to be simple.
The length of a path is the total number of vertices in a vertex sequence defining the path minus one, which is the same as the number of edges in the path. For example, a, c, b, f is a simple path oflength 3 from a to fin the graph of Figure 1.6a, whereas a, c, e, c, b, f is a path (not simple) oflength 5 from a to f. 

In the case of a directed graph, we are usually interested in directed paths. A directed path is a sequence of vertices in which every consecutive pair of the vertices is connected by an edge directed from the vertex listed first to the vertex listed next. 

A graph is said to be connected if for every pair of its vertices u and v there is a path from u to v. Informally, this property means that if we make a model of a connected graph by connecting some balls representing the graph's vertices with strings representing the edges, it will be a single piece. If a graph is not connected, such a model will consist of several connected pieces that are called connected
components of the graph. Formally, a connected component is a maximal (not expandable via an inclusion of an extra vertex) connected subgraph3 of a given graph. 

Trees
A tree (more accurately, afree tree) is a connected acyclic graph (Figure l.lOa). A graph that has no cycles but is not necessarily connected is called a forest: each of its connected components is a tree 

Rooted trees Another very important property of trees is the fact that for every two vertices in a tree, there always exists exactly one simple path from one of these vertices to the other. This property makes it possible to select an arbitrary vertex in a free tree and consider it as the root of the so-called rooted tree. A rooted tree is usually depicted by placing its root on the top (level 0 of the tree), the vertices
adjacent to the root below it (Ievell), the vertices two edges apart from the root below that (level2), and so on. Figure 1.11 presents such a transformation from a free tree to a rooted tree.


Rooted trees play a very important role in computer science, a much more
important one than free trees do; in fact, for the sake of brevity, they are often referred to as simply "trees." Obvious applications of trees are for describing hierarchies, from file directories to organizational charts of enterprises. There are many less obvious applications, such as implementing dictionaries (see below), efficient storage of very large data sets (Section 7.4), and data encoding (Section 9.4). As we discuss in Chapter 2, trees also are helpful in analysis of recursive algorithms. To finish this far-from-complete list of tree applications, we should mention the so-called state-space trees that underline two important algorithm design techniques: backtracking and branch-and-bound

For any vertex v in a tree T, all the vertices on the simple path from the root to that vertex are called ancestors of v. The vertex itself is usually considered its own ancestor; the set of ancestors that excludes the vertex itself is referred to as proper ancestors. If (u, v) is the last edge of the simple path from the root to vertex v (and
u ,P v), u is said to be the parent of v and vis called a child of u; vertices that have the same parent are said to be siblings. A vertex with no children is called a leaf; a vertex with at least one child is called parental. All the vertices for which a vertex v is an ancestor are said to be descendants of v; the proper descendants exclude
the vertex v itself. All the descendants of a vertex v with all the edges connecting them form the subtree ofT rooted at that vertex. Thus, for the tree of Figure 1.11b, the root of the tree is a; vertices d, g, .f, h, and i are leaves, while vertices a, b, e,
and c are parental; the parent of his a; the children of b are c and g; the siblings of bared and e; the vertices of the subtree rooted at bare {b. c, g, h, i).

The depth of a vertex vis the length of the simple path from the root to v. The height of a tree is the length of the longest simple path from the root to a leaf. For example, the depth of vertex c in the tree in Figure 1.1lb is 2, and the height of the tree is 3. Thus, if we count tree levels top down starting with 0 for the root's level, the depth of a vertex is simply its level in the tree, and the tree's height is the
maximum level of its vertices. (You should be alert to the fact that some authors define the height of a tree as the number of levels in it; this makes the height of a tree larger by 1 than the height defined as the length of the longest simple path from the root to a leaf.) 

Ordered trees An ordered tree is a rooted tree in which all the children of each vertex are ordered. It is convenient to assume that in a tree's diagram, all the 1·.• children are ordered left to right. A binary tree can be defined as an ordered tree in which every vertex has no more than two children and each child is designated as either a left child or a right child of its parent. The subtree with its root at the left (right) child of a vertex is called the left (right) subtree of that vertex.